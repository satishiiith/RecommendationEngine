{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Collaborative filtering Presentation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q11osqBwaTdM","colab_type":"text"},"source":["# COLLABORATIVE FILTERING\n"," \n","**Suppose for user A and user B, specified rating are correlated (which can be identfied by algorithm) then it is very likely that the ratings in which only one of the them is specified, are also likely to similar. This similarity can be used to make inferences about incompletely specified value**. This is the main idea of collaborative filtering. There are two types of methods that are commonly used in collaborative filtering, which are referred to as memory based methods and model based methods.\n","\n","Collaborative filtering models use the collaborative power of ratings given by multiple users to make recommendations.\n","\n","Let $m$ represents number of users and $n$ represents number of items. Most users have rated only a fraction of items. As a result most of the ratings will be not specified. Let $r_{ij}$ represent the rating of user $i$ and item $j$. Let R represent the rating matrix with $r_{ij}$ as elements. Point to note is **rating matrix R is incomplete**. An example of ratings matrix given in figure.\n","\n","![](https://drive.google.com/uc?export=view&id=1YNzJz5zSD1YiceAOMtPekx7jUUzBDEGA)\n","\n","\n","## MEMORY BASED COLLABORATIVE FILTERING\n","\n","In this ratings on user-item combinations are predicted on basis of their neighborhoods. Because of different type of neighborhoods we have two types here\n","\n","### 1) User based\n","In this case the ratings provided by similar users to a target user A are used to make recommendations for A. The predicted ratings for A are computed as **weighted average** of the group that are most similar to A. \n","\n","Let $I_{u}$ denote the set of item indices denoted by user $u$. One way to measure similarity between two users is to use **Pearson correlation** between the ratings given by them. We can also other similarity functions like cosine similarity but we will stick to this one here. The pearson correlation between two users $u$ and $v$ is given by \n","\n","$$\n","sim(u,v) =  \\frac{\\sum_{k\\in I_{u} \\cap I{v}}(r_{uk}-\\mu_{u})(r_{vk}-\\mu_{v})}{\\sqrt{\\sum_{k\\in I_{u} \\cap I{v}}(r_{uk}-\\mu_{u})^{2}}\\sqrt{\\sum_{k\\in I_{u} \\cap I{v}}(r_{vk}-\\mu_{v})^{2}}}\n","$$\n","\n","Here $\\mu_{u}$ and $\\mu_{v}$ are the respective avergaes of the ratings for user $u$ and $v$. Note the average here is computed only over the items that are both common to $u$ and $v$. \n","\n","For example, user $u$ has ratings $[1,3,4,7,6]$ on items and user $v$ has ratings $[2,1,3,?,?]$, then pearson correlation betweem them is \n","\n","$$\n","sim = \\frac{(1-2.66)(2-2)+(3-2.66)(1-2)+(4-2.66)*(3-2)}{\\sqrt{(1-2.66)^2+(3-2.66)^2+(4-2.66)^2}\\sqrt{(2-2)^2+(1-2)^2+(3-2)^2}}\n","$$\n","\n","After finding the similiarity between target user and all the users, now can on select top-k users based on similarity. The weighted average of these ratings can be returned as the predicted rating for that item with weights being equal to similarities. Before this we preprocess the rating matrix by **mean centering in row wise fashion**. We can write \n","$$\n","s_{uj} = r_{uj} - \\mu_{u}\n","$$\n","\n","Now prediction will be given by\n","$$\n","\\hat{r_{ij}} = \\mu_{u} + \\frac{\\sum_{u\\in P_{u}(j)}sim(u,v)s_{u,j}}{\\sum_{u\\in P_{u}(j)}\\lvert sim(u,v) \\rvert}\n","$$\n","\n","Here $P_{u}(j)$ is the top-k neighbour of user $u$ for item $j$.\n","\n","\n","The psuedo code for the algorithm will be\n","```\n","Compute the similarity between users (cosine or pearson)\n","For a given user select top-k neighbours. (by similarity)\n","Rating of user i on item j is predicted as weighted average of the top-k neighbours.\n","```\n","\n","### 2) Item Based\n","In order to make recommendations for target item B, we will deterime the set S of items that are most similar to B. Then in order to predict the rating of any particular user A for item B, the ratings in set S, which are specified by A. The weighted average of these ratings are used as prediction. \n","\n","The important distinction is that in item based we will find the **similarties between items** rather than **similarites between users**. The predicted rating will be\n","$$\n","\\hat{r_{ut}} = \\frac{\\sum_{j \\in Q_{t}(u)}sim(j,t)r_{uj}}{\\sum_{j \\in Q_{t}(u)}\\lvert sim(j,t) \\rvert}\n","$$\n","\n","The psuedo code for the algorithm will be \n","```\n","Compute the similarity between each item pair.\n","For a given item, select the top-k items\n","Rating for user i and item j, is weighted average of ratings given by i in the neighbours.\n","```\n","\n","##### Comparision between user based and item based\n","- Item-based methods often provide more relevant recommendations because of the fact that a user’s own ratings are used to perform the recommendation. As in item-based methods, similar items are identified to a target item, and the user’s own ratings on those items are used to extrapolate the ratings of the target user.\n","\n","- We can give explanations or interpret item based recommendations. On the other hand, these explanations are harder to address withuser-based methods, because the peer group is simply a set of anonymous users and not directly usable in the recommendation process.\n","\n","- Finally, item-based methods are more stable with changes to the ratings.\n","\n","#### STRENGTHS AND WEAKNESS OF MEMORY BASED MODELS\n","\n","- Because of the simple and intuitive approach of these methods, they are easy to implement and debug.Because of the simple and intuitive approach of these methods, they are easy to implement and debug.\n","\n","- Main disadvantage of these methods is sparsity. As sparse ratings matrices are more common, finding neighbours will be tough and this will make the prediction not very accurate.Sparsity also creates challenges for robust similarity computation when the number of mutually rated items between two users is small.\n","\n","\n","\n","## MODEL BASED COLLABORATIVE FILTERING\n","\n","In model-based methods, a summarized model of the data is created up front, as with supervised or unsupervised machine learning methods. Therefore, the training (or model-building phase) is clearly separated from the prediction phase. Even though neighborhood-based methods were among the earliest collaborative filtering methods and were also among the most popular because of their simplicity, they are not necessarily the most accurate models available today. In fact, some of the most accurate methods are based on model-based techniques in general, and on latent factor models in particular.\n","\n","### LATENT FACTOR MODELS\n","\n","In this method, we use dimensionality reduction method to directly estimate the matrix in one shot. The basic idea is to exploit the fact that significant portions of rows and columns of data matrices are highly correlated. As a result, the data has built-in redundancies and the resulting data matrix is often approximated quite well by **low rank matrix**.  \n","\n","Latent factor models are considered to be state of the art in recommender systems. These models leverage well known dimensionality reduction methods to fill the missing entries.\n","\n","Factorization is a more general way of approximating a matrix when it is prone to dimensionality reduction because of correlations between columns (or rows). Most dimensionality reduction methods can also be expressed. Matrix factorization methods provide a neat way to leverage all row and column correlations in one shot to estimate the entire data matrix. This sophistication of the approach is one of the reasons that latent factor models have become the state-of-the-art in collaborative filtering.\n","\n","\n","The key idea is that **any $m \\times n$ matrix R of rank $k << \\, min(m,n)$ can always be expressed as following product.**\n","\n","$$\n","R = UV^{T}\n","$$\n","\n","Here U is $m \\times k$ matrix and V is $n \\times k$ matrix. Even when the matrix R has rank larger than k, it can often be approximately expressed as the product of rank-k factors\n","$$\n","R \\approx UV^{T}\n","$$\n","\n","let us consider the straightforward case in which R is fully observed. If the matrix R is fully specified, then are there are lot of methods to find U and V. But what happens **if we missing entries in R ?**. The main idea here is to **minimize the errors between only observed entries given by R and product $UV^{T}$**. So we have written this as an optimization problem. Once solved , that is if we can find U and V that minimize the errors, **we can estimate the matrix as $UV^{T}$ in one shot**.\n","\n","One can formulate the optimization problem with respect to matrices U and V in order to acheive this goal \n","$$\n","\\min_{U,V} J = \\frac{1}{2}\\displaystyle\\sum_{(i,j)\\in S}(r_{i,j}-\\displaystyle\\sum_{s=1}^{k} u_{is}v_{js})^{2}\n","$$\n","\n","where S denotes the set of $(i,j)$ for which $r_{ij}$ is observed. A varitey of **gradient methods** can be used to provide an optimal solution to this factorization.\n","\n","\n","#### REGULARIZATION\n","\n","One of the main problems with this approach arises when the ratings matrix R is sparse and relatively few entries are observed. In such cases when observed set is small, overfitting will arise. A common apporach to address overfitting is to use ***regularization***. Here we $l_{2}$ regularization and the main idea is to add square of parameters to the objective.\n","\n","In case of regularization, our objective will become\n","$$\n","\\min_{U,V} J = \\frac{1}{2}\\displaystyle\\sum_{(i,j)\\in S}(r_{i,j}-\\displaystyle\\sum_{s=1}^{k} u_{is}v_{js})^{2} + \\frac{\\lambda}{2}\\displaystyle\\sum_{i=1}^{m}\\displaystyle\\sum_{s=1}^{k}u_{is}^{2} + \\frac{\\lambda}{2}\\displaystyle\\sum_{j=1}^{n}\\displaystyle\\sum_{s=1}^{k}v_{js}^{2}\n","$$\n","\n","The basic idea is to create a bias in favor of simpler solutions by penalizing large coefficients. This is a standard approach, which is used in many forms of classification and regression, and also leveraged by collaborative filtering. The parameter $\\lambda$ is always non-negative and it controls the weight of the regularization\n","term.\n","\n","Value of $\\lambda$ can be found by using a hold out set and finding the $\\lambda$ which will have minimum error on hold out set.\n","\n","#### STOCHASTIC GRADIENT DESCENT\n","\n","We can find the gradient of the above function and can use gradient descent to optimize. But in practice, faster convergence is chieved by the stochastic gradient descent method as compared to the batch method.\n","\n","The algorithm is given below\n","\n","```\n","Rating Matrix R\n","begin\n","    Randomly initialize U and V;\n","    while not(covergence) do\n","    begin\n","        randomly shuffle entries in S;\n","        for each (i,j) in s\n","        begin\n","            error = r - u.v\n","            find gradient\n","            u = u - lr*gradu\n","            v = v - lr*gradv\n","        end\n","        check convergence condition\n","    end\n","end\n","```\n","\n","This algorithm is same as we do stochastic gradient descent in neural networks. Point to note, final solution will depend on initialization and learning rate of the algorithm.\n","\n","\n","#### ALTERNATING LEAST SQUARES (ALS)\n","The stochastic gradient method is an efficient methodology for optimization. On the other hand, it is rather sensitive, both to the initialization and the way in which the step sizes are chosen. Other methods for optimization include the use of alternating least squares(ALS) which is generally more stable.\n","\n","The basic idea of the apporach is\n","1. Keeping U fixed, we solve for each of $n$ rows of V by treating the problem as least square regression problem. As we have to solve $n$ least square problem and each problem is independent of other, we can parallelize this step easily.\n","2. Keeping V fixed, we solve for each of $m$ rows of U by treating the problem as least square regerssion problem. As we have to solve $m$ least square problem and each problem is independent of other, we can parallelize this step as in previous case.\n","\n","\n","These two steps are iterated to convergence which typically occurs within a small (< 20) number of iterations even for very large matrices consisting of tens of millions of rows or columns. We can also use regularization here as it will be equivalent solving linear regression problem with regularization.\n","\n","##### WEIGHTED ALERNATING LEAST SQUARES (WALS)\n","\n","Impilicit matrices meaning the matrix is filled with implicit rating. In this cases, customer\n","preferences are derived from their activities rather than their explicitly specified ratings. For example, the buying behavior of a customer is implicit. Similarly, many social networks, such as Facebook, use “like” buttons, which provide the\n","ability to express liking for an item. \n","\n","This method is usually **used for implicit rating matrices** in which the matrix is assumed to be fully specified with many zero values. When lot of entries are zero some tricks can be used to make weighted ALS efficient.\n","\n","\n","It introduces the weights for zero(unobserved) entries, and observed entries. \n","The objective will be\n","\n","$$\n","\\min_{U,V} J = \\frac{1}{2}\\displaystyle\\sum_{(i,j)\\in S}w(r_{i,j}-\\displaystyle\\sum_{s=1}^{k} u_{is}v_{js})^{2}\n","$$\n","here\n","- $w_{is} = w_{0}$   for unobserved entries\n","- $w_{is} = w_{0} + f(c_{i})$ for observed entries, $c_{i}$ is the number of non zero entries in row $i$.\n","\n","This type of weighting allows for a more flexible model of the user's preferences and produces better empirical results than the unweighted version. The function $f$ varies according to the dataset and whether ratings are explicit or implicit.\n","\n","\n","If an item is not seen during training, the system can't create an embedding for it and can't query the model with this item. This issue is often called the **cold-start problem**. WALS handles cold start problem. Given a new item $i_{0}$ not seen in training, if the system has a few interactions with users, then the system can easily compute an embedding  for this item without having to retrain the whole model."]}]}